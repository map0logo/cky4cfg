{
 "metadata": {
  "name": "",
  "signature": "sha256:dfc0615e01566d4fa39ebb535e4c4ca2ddd85e4f5042eddbb38a3efcdcd5beb7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Assignment 3\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The objective is\n",
      "> To implement Cocke-Kasami-Younger (CKY) algorithm for bottom-up CFG parsing, and apply it to the word and the parsing problem of English.\n",
      "\n",
      "Ingredients that we have to represent or manipulate using NLTK, are:\n",
      "- Grammar\n",
      "- Test sentences\n",
      "- CKY algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Few notes about Assignment statement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To run examples from the statement of the assignment you need *atis* grammar, if atis grammar is not available, execute `nltk.download()` and select *large_grammars* in tab *models* and press \"install\".\n",
      "\n",
      "The following line of the statement is wrong!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse all test sentences\n",
      "for sentence in s:\n",
      "    parser.chart_parse(sentence[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's because `s` is a long string with all the sentences joined. You have to use `t` that is a list of tuples, where each element of the list corresponds to a sentence, a tuple whose first element is a list of words in the sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse all test sentences\n",
      "for sentence in t:\n",
      "    try:\n",
      "        parser.chart_parse(sentence[0])\n",
      "    except ValueError:\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I explain this because is quite frustrating to try the most basic commands and that they don't work.\n",
      "\n",
      "The `try-except` statement it is necessary because some sentences had tokens that are not available in the grammar, this rise an exception that stops program execution. This statement prevents program execution stops.\n",
      "\n",
      "This is a mistake in the assignment wording."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To make easier the execution, move `atis-grammar-cnf.cfg` and `atis-test-sentences.txt` to the same folder as the script. They are modified versions of files that can be downloaded using `ntlk.download()` and that are located on `nltk_data` folder of your home.\n",
      "\n",
      "Files of *atis* `large-grammar` are useful because `atis_sentences.txt` files has the number of parse trees corresponding to each sentence, this information is handy to check our algorithms."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Loading grammar and sentences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, let's start!\n",
      "Begin loading the grammar:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "g = nltk.data.load(\"atis-grammar-cnf.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(g)\n",
      "g"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'nltk.grammar.CFG'>\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<Grammar with 20326 productions>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the parameter of `nltk.data.load()` has the `.cfg` extension, NLTK read it as a grammar."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then load the sentences:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = nltk.data.load(\"atis_test_sentences_a3.txt\", \"raw\")\n",
      "s[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'what is the cheapest one way flight from columbus to indianapolis .\\nis there a flight from memphis t'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = nltk.parse.util.extract_test_sentences(s)\n",
      "t[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[(['what',\n",
        "   'is',\n",
        "   'the',\n",
        "   'cheapest',\n",
        "   'one',\n",
        "   'way',\n",
        "   'flight',\n",
        "   'from',\n",
        "   'columbus',\n",
        "   'to',\n",
        "   'indianapolis',\n",
        "   '.'],\n",
        "  None),\n",
        " (['is',\n",
        "   'there',\n",
        "   'a',\n",
        "   'flight',\n",
        "   'from',\n",
        "   'memphis',\n",
        "   'to',\n",
        "   'los',\n",
        "   'angeles',\n",
        "   '.'],\n",
        "  None),\n",
        " (['please',\n",
        "   'show',\n",
        "   'me',\n",
        "   'the',\n",
        "   'flights',\n",
        "   'from',\n",
        "   'chicago',\n",
        "   'to',\n",
        "   'detroit',\n",
        "   'that',\n",
        "   'arrive',\n",
        "   'at',\n",
        "   'six',\n",
        "   'p.m.',\n",
        "   'next',\n",
        "   'tuesday',\n",
        "   '.'],\n",
        "  None)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, the function `nltk.parse.util.extract_test_sentences()` splits `s` by `\\n` as separator of sentences, and then split each sentence using whitespace as separator of words.\n",
      "\n",
      "`None` values indicate that there are not information on whether the sentence is grammatical or not, or the number of parse trees that should have."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Production table"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we create a `production_table` that is a dict of lists such that the keys are productions right-hand-sides of the given grammar, and the values the corresponding production.\n",
      "\n",
      "This a little trick that would be necessary for `CKY` algoritms implementations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from assignment3 import init_production_table\n",
      "init_production_table??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pt = init_production_table(g)\n",
      "print pt[('VERB_VB', 'BUM')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[IMPR_VB -> VERB_VB BUM, SIGMA -> VERB_VB BUM]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##CKY recognizer\n",
      "\n",
      "In Jurafsky and Martin (2009) we can get the following pseudocode for the CKY algorithm:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function CKY-PARSE (words, grammar) returns table\n",
      "    for j \u2190 from 1 to LENGTH(words) do\n",
      "        table[j \u2212 1, j] \u2190 {A | A \u2192 words[j] \u2208 grammar }\n",
      "        for i \u2190 from j \u2212 2 downto 0 do\n",
      "            for k \u2190 i + 1 to j \u2212 1 do\n",
      "                table[i,j] \u2190 table[i, j] \u222a\n",
      "                    {A | A \u2192 BC \u2208 grammar, B \u2208 table[i, k], C \u2208 table[k, j]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then the implementation is almost straightforward in Python using NLTK objects."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from assignment3 import cky_recognizer\n",
      "cky_recognizer??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In `cky_recognizer()` we start to define the table:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    n_tokens = len(tokens)\n",
      "    table = [[None for j in range(n_tokens + 1)] for i in range(n_tokens)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"words\" in the pseudocode corresponds to `tokens`, and \"grammar\" corresponds to `cnf_grammar`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The loop \n",
      "\n",
      "    for j \u2190 from 1 to LENGTH(words) do\n",
      "\n",
      "is implemented as:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    for j in range(1, n_tokens + 1):"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`LENGTH(words)` corresponds to `n_tokens`.\n",
      "\n",
      "Remember that `range(i, j)` goes from `i` to `j-1`, therefore is added one to n_tokens."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The expression:\n",
      "\n",
      "        table[j \u2212 1, j] \u2190 {A | A \u2192 words[j] \u2208 grammar }\n",
      "\n",
      "Means that left-hand-sides of the productions with `word[j]` in right-hand-side that belong to the grammar are assigned at the position `[j - 1, j]` of the table (the diagonal)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        productions = cnf_grammar.productions(rhs=tokens[j-1]) # productions with tokens[j-1] (word[j]) in rhs\n",
      "        lhs_list = [] # list of left-hand-side\n",
      "        for production in productions:\n",
      "            lhs_list.append(production.lhs())\n",
      "        table[j - 1][j] = lhs_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The loop\n",
      "\n",
      "        for i \u2190 from j \u2212 2 downto 0 do\n",
      "\n",
      "Corresponds to:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        for i in range(j - 2, -1, -1):"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And\n",
      "\n",
      "            for k \u2190 i + 1 to j \u2212 1 do\n",
      "                table[i,j] \u2190 table[i, j] \u222a\n",
      "                    {A | A \u2192 BC \u2208 grammar,\n",
      "                         B \u2208 table[i, k],\n",
      "                         C \u2208 table[k, j]}\n",
      "\n",
      "Means that left-hand-sides of the productions with B and C in right-hand-side that belong to the grammar, are assigned at the position `[i, j]` of the table (upper right corner). Where, B and C are elements to the \"right\" and \"below\" of position `[i, j]` respectively.\n",
      "\n",
      "> \"the contens of two cells can be combined in a way that is sanctioned by a rule in the grammar. If such a rule exists, the non-terminal on its left-hand side is entered into the table.\" (Jurafsky & Martin, 2009)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "            lhs_list = []\n",
      "            # Range over the places the string can be split\n",
      "            for k in range(i + 1, j):\n",
      "                right = table[i][k] # Right along row i\n",
      "                down = table[k][j] # Down along column j\n",
      "                for r_lhs in right:\n",
      "                    for d_lhs in down:\n",
      "                        key = (str(r_lhs), str(d_lhs))\n",
      "                        if production_table.has_key(key):\n",
      "                            for production in production_table[key]:\n",
      "                                lhs_list.append(production.lhs())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here you can see the `production_table` trick. It's an easy way to obtain the productions that corresponds to a given pair of nonterminals."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, the function returns the table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    return table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Is in CFG language"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To determine whether the sentence is in the CKY language we can use `is_in_CFG_language()` function.\n",
      "\n",
      "This function basically verifies that at least one nonterminal in the upper right corner of the table corresponds to the grammar start (in this case `'SIGMA'`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from assignment3 import is_in_CFG_language\n",
      "is_in_CFG_language??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try 40 sentences from `atis-test-sentences.txt`, that are selected in the file `\"atis_test_sentences_a3.txt\"`. Sentences with more than 50 trees were avoided. There are 3 cases:\n",
      "\n",
      "- Sentences that are in the language\n",
      "- Sentences not recognized by language\n",
      "- Sentences whose tokens are not covered by grammar"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, sentence in enumerate(t):\n",
      "    table = cky_recognizer(sentence[0], g, pt)\n",
      "    if table != []:\n",
      "        if is_in_CFG_language(table, g):\n",
      "            print 'sentence {} is in language'.format(i + 1)\n",
      "        else:\n",
      "            print 'sentence {} not recognized'.format(i + 1)\n",
      "    else:\n",
      "        print 'sentence {} has tokens not covered by grammar'.format(i + 1)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sentence 1 is in language\n",
        "sentence 2 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 3 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 4 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 5 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 6 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 7 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 8 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 9 is in language\n",
        "sentence 10 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 11 is in language\n",
        "sentence 12 is in language\n",
        "sentence 13 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 14 not recognized\n",
        "sentence 15 is in language\n",
        "sentence 16 has tokens not covered by grammar\n",
        "sentence 17 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 18 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 19 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 20 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 21 has tokens not covered by grammar\n",
        "sentence 22 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 23 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 24 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 25 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 26 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 27 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 28 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 29 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 30 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 31 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 32 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 33 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 34 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 35 has tokens not covered by grammar\n",
        "sentence 36 not recognized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 37 not recognized\n",
        "sentence 38 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 39 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sentence 40 is in language"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Examples:\n",
      "\n",
      "- Sentence 4: *\"does united flight four seven four slash fourteen eighty four serve dinner .\"* is not recognized, because none of the nonterminals of the upper level is the start of grammar.\n",
      "- Sentence 16: *\"list these city destinations .\" has the token \"destinations\"* that is not covered by the grammar."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## CKY parser"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main diference between the recognizer and the parser is that where before we had left sides of productions now have trees (of type `nltk.Tree`).\n",
      "\n",
      "Where we had in recognizer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        productions = cnf_grammar.productions(rhs=tokens[j-1])\n",
      "        lhs_list = []\n",
      "        for production in productions:\n",
      "            lhs_list.append(production.lhs())\n",
      "        table[j - 1][j] = lhs_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In parser we have:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        productions = cnf_grammar.productions(rhs=tokens[j-1])\n",
      "        trees = []\n",
      "        for production in productions:\n",
      "            trees.append(nltk.Tree(production.lhs(), [production.rhs()]))\n",
      "        table[j-1][j] = trees"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the tree is an structure such that the parent node corresponds to the production left-hand-side, and the children nodes corresponds to the production right-hand-side.\n",
      "\n",
      "In a similar way, where in recognizer we had:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "            lhs_list = []\n",
      "            # Range over the places the string can be split\n",
      "            for k in range(i + 1, j):\n",
      "                right = table[i][k] # Right along row i\n",
      "                down = table[k][j] # Down along column j\n",
      "                for r_lhs in right:\n",
      "                    for d_lhs in down:\n",
      "                        key = (str(r_lhs), str(d_lhs))\n",
      "                        if production_table.has_key(key):\n",
      "                            for production in production_table[key]:\n",
      "                                lhs_list.append(production.lhs())\n",
      "            table[i][j] = lhs_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, in parser we have:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "            trees = []\n",
      "            # Range over the places the string can be split\n",
      "            for k in range(i + 1, j):\n",
      "                right = table[i][k] # Right along row i\n",
      "                down = table[k][j] # Down along column j\n",
      "                for r_tree in right:\n",
      "                    for d_tree in down:\n",
      "                        key = (str(r_tree.label()), str(d_tree.label()))\n",
      "                        if production_table.has_key(key):\n",
      "                            for production in production_table[key]:\n",
      "                                trees.append(nltk.Tree(production.lhs(),\n",
      "                                                       [r_tree, d_tree]))\n",
      "            table[i][j] = trees                                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this last snippet of code, note that the magic is that `r_tree` and `d_tree` are inserted as children nodes, so that at the top level we have complete trees for the whole sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    trees_found = []\n",
      "    for tree in table[0][n_tokens]:\n",
      "        if str(tree.label()) == str(cnf_grammar.start()):\n",
      "            trees_found.append(tree)\n",
      "\n",
      "    return trees_found"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, we return a list of trees, those which top node coincides with the grammar start (`'SIGMA'`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from assignment3 import cky_parser\n",
      "cky_parser??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"atis_n_trees.txt\", \"w\")\n",
      "for sentence in t:\n",
      "    trees = cky_parser(sentence[0], g, pt)\n",
      "    f.write(\"{}\\t{}\\n\".format(len(trees),' '.join(sentence[0])))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trees = cky_parser(t[5][0], g, pt)\n",
      "for tree in trees:\n",
      "    tree.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}